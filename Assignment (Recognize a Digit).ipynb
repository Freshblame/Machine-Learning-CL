{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0126729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install & Import Packages\n",
    "\n",
    "!pip install torch torchvision matplotlib --quiet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b00d06d",
   "metadata": {},
   "source": [
    "* !pip install ...--> Installs the required Python libraries.\n",
    "\n",
    "* torch --> The main PyTorch deep learning framework.\n",
    "\n",
    "* torchvision --> Contains datasets and image processing utilities.\n",
    "\n",
    "* matplotlib --> Used for plotting and displaying images.\n",
    "\n",
    "* --quiet --> Suppresses extra installation output to keep the notebook clean.\n",
    "\n",
    "* import torch → Loads PyTorch into memory.\n",
    "\n",
    "* import torch.nn as nn → Lets us define neural network layers.\n",
    "\n",
    "* import torch.optim as optim → Gives access to optimization algorithms (like Adam, SGD).\n",
    "\n",
    "* import torchvision → Lets us access datasets (MNIST), transforms, and models.\n",
    "\n",
    "* import torchvision.transforms as transforms → Used to process/transform images before feeding them to the model.\n",
    "\n",
    "* import matplotlib.pyplot as plt → For displaying images and plots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bdcdf28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transform: Convert to tensor and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Download MNIST dataset\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# Show 5 sample images\n",
    "images, labels = next(iter(train_loader))\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 2))\n",
    "for i in range(5):\n",
    "    axes[i].imshow(images[i].squeeze(), cmap='gray')\n",
    "    axes[i].set_title(f\"Label: {labels[i]}\")\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712fd6aa",
   "metadata": {},
   "source": [
    "* transforms.Compose([...]) → Combines multiple image transformations.\n",
    "\n",
    "* transforms.ToTensor() → Converts a PIL image or NumPy array into a PyTorch tensor (0–1 range).\n",
    "\n",
    "* transforms.Normalize((0.5,), (0.5,)) → Normalizes pixel values to be between -1 and 1, which helps training.\n",
    "\n",
    "* Loads the training MNIST dataset.\n",
    "\n",
    "* root='./data' → Saves the dataset in the \"data\" folder.\n",
    "\n",
    "* train=True → Specifies that we want the training split.\n",
    "\n",
    "* download=True → Downloads it if it’s not already saved.\n",
    "\n",
    "* transform=transform → Applies our preprocessing (ToTensor + Normalize).\n",
    "\n",
    "* Creates a data loader to feed data in batches to the model.\n",
    "\n",
    "* batch_size=32 → 32 images per training step.\n",
    "\n",
    "* shuffle=True → Randomizes the order for better training.\n",
    "\n",
    "* Loads the test MNIST dataset (no shuffling, since testing is in fixed order).\n",
    "\n",
    "* iter(train_loader) → Creates an iterator over the training data.\n",
    "\n",
    "* next(...) → Gets the first batch (images + labels).\n",
    "\n",
    "* Creates a row of 5 plots (1, 5 means 1 row, 5 columns).\n",
    "\n",
    "* figsize=(10, 2) → Sets the size in inches.\n",
    "\n",
    "* imshow(...) → Displays the image.\n",
    "\n",
    "* .squeeze() → Removes extra dimensions (from 1×28×28 to 28×28).\n",
    "\n",
    "* cmap='gray' → Displays the image in grayscale.\n",
    "\n",
    "* set_title(...) → Shows the digit label above each image.\n",
    "\n",
    "* axis('off') → Hides the axis.\n",
    "\n",
    "* plt.show() → Displays the plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56401b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "\n",
    "class DigitClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DigitClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = DigitClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11696e82",
   "metadata": {},
   "source": [
    "* class DigitClassifier(nn.Module) → Creates a custom neural network.\n",
    "\n",
    "* super(...).__init__() → Initializes the base nn.Module class.\n",
    "\n",
    "* nn.Linear(in_features, out_features) → A fully connected layer.\n",
    "\n",
    "  28*28 → Flattened size of MNIST images.\n",
    "\n",
    "  128, 64 → Number of neurons in hidden layers.\n",
    "\n",
    "  10 → Output neurons for digits 0–9.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57039ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 0.0374\n",
      "Epoch 2/2, Loss: 0.1733\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 2  # Short training for speed\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d42e834",
   "metadata": {},
   "source": [
    "* epochs → Number of training passes over the dataset.\n",
    "\n",
    "* optimizer.zero_grad() → Clears old gradients.\n",
    "\n",
    "* model(images) → Runs images through the model.\n",
    "\n",
    "* loss = criterion(...) → Calculates the error.\n",
    "\n",
    "* loss.backward() → Computes gradients via backpropagation.\n",
    "\n",
    "* optimizer.step() → Updates model weights.\n",
    "\n",
    "* print(...) → Shows training progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7461b60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.27%\n"
     ]
    }
   ],
   "source": [
    "# Test Model\n",
    "\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c6f34",
   "metadata": {},
   "source": [
    "* torch.no_grad() → Disables gradient calculation (faster, less memory).\n",
    "\n",
    "* torch.max(outputs, 1) → Finds predicted label with highest score.\n",
    "\n",
    "* labels.size(0) → Number of samples in batch.\n",
    "\n",
    "* (predicted == labels) → Compares predictions with actual labels.\n",
    "\n",
    "* sum().item() → Counts correct predictions.\n",
    "\n",
    "* Prints accuracy in percentage.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
